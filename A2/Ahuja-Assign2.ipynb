{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2 | Applied Machine Learning | Paras Ahuja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. At a high-level, without entering into mathematical details, compare and contrast the following classifiers: \n",
    "\n",
    "* Perceptron: In the book Perceptron algorithm was used to minimize classification error. We are simply updating the weights until a class is correctly classified. It is rather simplistic in its approach. For Perceptron we randomly initialize weights, then we compute the linear combination of an example with randomly initialized weights, and finally this is passed to a threshold function, if our predicted class is correct, then we do not update the weights, however it is incorrect we compute the delta weight, which is $\\eta \\times(y^{(i)} - \\hat{y}^{(i)}) \\times x_j^i$, and simultaneously update the weights. Perceptron works well when classes are linearly separable. However, if they are not then we have to provide hyperparameters, which allow us to tune, and set a limit to the number of iterations before we stop. One of the hyperparameters is theta, which in the book has been randomly assigned. It is rather difficult to classify input vectors that are not linearly separable. Single layer Perceptron is very fast and computationally less expensive when it comes it simple problems, however for more complex problems other algorithms might be better suited.\n",
    "\n",
    "\n",
    "* SVM: Support Vector Machine is considered as an extension of Perceptron. In SVM we want to maximize the margins around the hyperplane, which is the decision boundary between the the classes. The objective function or the optimization function for SVM is $\\frac{2}{||w||}$, and our goal is to maximize this. This also means that we want to minimize $||w||$. However, in practice we minimize the term $\\frac{1}{2}||w||^2$. SVM is robust in the sense that it helps us curtail some of the overfitting issues. When we maximize the margins, what we are really doing is ensuring that large margins tend to have lower generalization error, however models with small margins are more prone to overfitting the data. If we overfit the data we are in high variance state. SVM also works with non-linearly separable data. In this instance it projects the data onto a higher plane using kernel methods. SVM is definitely much more robust than Perceptron.\n",
    "\n",
    "\n",
    "* Decision Tree: Decision tree classifiers have an advantage when it comes to interpretability of the data. First thing we optimize here is the information gain. We split the data on the node that results in the largest information gain. This process is repeated until we are only left with leaves. Also, note that there are 3 splitting criteria. We can use Gini impurity, entropy, and classification error. Decision trees are very robust, and we can build complex decision boundaries. Also there are libraries like graphviz available that allow us to visualize our tree. There is potential of overfitting as well as the tree gets deeper, so we can set maximum depth or start pruning the tree. \n",
    "\n",
    "\n",
    "* Random Forest: Random forsests stem from decision trees. These are also very robust models. The idea behind random forests is to avergae the decision trees that individually suffer from overfitting. Random forests therefore have a better generalization performance and are less susceptible to overfitting the data. In comparison to decision trees random forests don't offer the same level of interpretability. Furthermore, one of the biggest advantage of random forests is that we don't have to worry so much about choosing good hyperparameter values. We do need to pay a close attention to the number of trees we want in our random forest. \n",
    "\n",
    "\n",
    "Which one of these algorithms should be used first depends on the type of data we have, and what criteria matters. For instance, if we have information about the data that the data is not linearly separable, then we should not think of using Perceptron, and instead move forward with SVM, but if iterpretability matters, then we have to use decision tree classifier. Even then, if our data is prone to overfitting then we move on to random forests, keeping in mind that we will not have the same level of interpretability. Therefore, which algorithm should be used first is rather subjective. However, if we are simply given a dataset and asked to create a model, we should start with the simplest first and then move upwards, in this case we will start with the Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using real datasets (can also be hypothetically constructed by yourself) define the following feature types, and give example values from your dataset. How would you represent these features in a computer program? (e.g. 32-bit integer? Floating point? String?\n",
    "\n",
    "* Numerical\n",
    "* Nominal\n",
    "* Date\n",
    "* Text\n",
    "* Image\n",
    "* Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows=1371, M columns=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance of Wavelet Transformed image</th>\n",
       "      <th>skewness of Wavelet Transformed image</th>\n",
       "      <th>curtosis of Wavelet Transformed image</th>\n",
       "      <th>entropy of image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.36840</td>\n",
       "      <td>9.6718</td>\n",
       "      <td>-3.9606</td>\n",
       "      <td>-3.16250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance of Wavelet Transformed image  \\\n",
       "0                                4.54590   \n",
       "1                                3.86600   \n",
       "2                                3.45660   \n",
       "3                                0.32924   \n",
       "4                                4.36840   \n",
       "\n",
       "   skewness of Wavelet Transformed image  \\\n",
       "0                                 8.1674   \n",
       "1                                -2.6383   \n",
       "2                                 9.5228   \n",
       "3                                -4.4552   \n",
       "4                                 9.6718   \n",
       "\n",
       "   curtosis of Wavelet Transformed image  entropy of image  class  \n",
       "0                                -2.4586          -1.46210      0  \n",
       "1                                 1.9242           0.10645      0  \n",
       "2                                -4.0112          -3.59440      0  \n",
       "3                                 4.5718          -0.98880      0  \n",
       "4                                -3.9606          -3.16250      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\")\n",
    "attributes = [\"variance of Wavelet Transformed image\", \"skewness of Wavelet Transformed image\", \"curtosis of Wavelet Transformed image\", \"entropy of image\", \"class\"]\n",
    "df.columns = attributes\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we have 525,461 instances in this data, and there are 8 features. Furthermore, there maybe some missing instances. UCI mostly marks the missing instances with a \"?\", and we will replace these and drop any rows with missing information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance of Wavelet Transformed image</th>\n",
       "      <th>skewness of Wavelet Transformed image</th>\n",
       "      <th>curtosis of Wavelet Transformed image</th>\n",
       "      <th>entropy of image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.36840</td>\n",
       "      <td>9.6718</td>\n",
       "      <td>-3.9606</td>\n",
       "      <td>-3.16250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance of Wavelet Transformed image  \\\n",
       "0                                4.54590   \n",
       "1                                3.86600   \n",
       "2                                3.45660   \n",
       "3                                0.32924   \n",
       "4                                4.36840   \n",
       "\n",
       "   skewness of Wavelet Transformed image  \\\n",
       "0                                 8.1674   \n",
       "1                                -2.6383   \n",
       "2                                 9.5228   \n",
       "3                                -4.4552   \n",
       "4                                 9.6718   \n",
       "\n",
       "   curtosis of Wavelet Transformed image  entropy of image  class  \n",
       "0                                -2.4586          -1.46210      0  \n",
       "1                                 1.9242           0.10645      0  \n",
       "2                                -4.0112          -3.59440      0  \n",
       "3                                 4.5718          -0.98880      0  \n",
       "4                                -3.9606          -3.16250      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace(\"?\", np.nan)\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1371 entries, 0 to 1370\n",
      "Data columns (total 5 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   variance of Wavelet Transformed image  1371 non-null   float64\n",
      " 1   skewness of Wavelet Transformed image  1371 non-null   float64\n",
      " 2   curtosis of Wavelet Transformed image  1371 non-null   float64\n",
      " 3   entropy of image                       1371 non-null   float64\n",
      " 4   class                                  1371 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 64.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data here is either nominal, ordinal, or binomial. We are not dealing with numeric data rather than a grading scale of sorts. Here is how these have been defined by UCI.\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous)\n",
    "2. skewness of Wavelet Transformed image (continuous)\n",
    "3. curtosis of Wavelet Transformed image (continuous)\n",
    "4. entropy of image (continuous)\n",
    "5. class (integer) \n",
    "\n",
    "\n",
    "   * Numerical: Numerical variable can be represented as either an int type or a float. We can easily perform numerical calculations on this data type, and find meaningful information. In this dataset we have variance of Wavelet Transformed image, skewness of Wavelet Transformed image, and curtosis of Wavelet Transformed image as numerical variable. As we can see these are definitely in float64 format.\n",
    "   * Nominal: Nominal data has to represented as an int type. Nominal data is defined as data that is used for naming or labelling variables, without any quantitative value. In our dataset, we do not have any nominal dataset.\n",
    "   * Date: Pandas has a datetime64[ns] type, therefore it is most likely to represented as this type. \n",
    "   * Text: will be represented a string type. We can use text information to establish a certain criteria like string containing a certain word.\n",
    "   * Image: We do not have image in our dataset, however it will represented as a matrix of number where the numbers would represent pixels. Similar to the one in module 1 digit analysis. \n",
    "   * Dependent Variable: Our dependent variable is generally a nominal data if we have a classification task, and it is numeric if the dataset is regression dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using online resources, research and find other classifier performance metrics which are also as common as the accuracy metric. Write down the mathematical equations and the meaning of the metrics that you found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:**\n",
    "\n",
    "This is the most commonly used metric, here we evaluate the total number of correct predictions (True Positive + True Negative) divided by the total number data examined (True Positive + False Positive + False Negative + True Negative). This is calculated as:\n",
    "\n",
    "$\\text{Accuracy} = \\frac{TP+TN}{TP+FP+FN+TN}$\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "Precision is a metric that we use when we are really interested in how much of our predictions are positive. We use precision metric when we want to be very certain about the prediction we have made. For instance, if we are dealing with a person's health information or financial data. We calculate this metric as: \n",
    "\n",
    "$\\text{Precision} = \\frac{TP}{TP+FP}$\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "With recall we are in interested in find out what proportion of actual Positives is correctly classified. We can also use this in tandem with Precision. We calculate this metric as: \n",
    "\n",
    "$\\text{Recall} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "**F1 Score:**\n",
    "\n",
    "F1 Score is a metric that finds a balance between precision and recall. We calculate this as: \n",
    "\n",
    "$F_1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "F1 Score is also widely used in the field of information retrieval for measuring search, document classification, and query classification performance. \n",
    "\n",
    "**Log Loss:**\n",
    "\n",
    "Log Loss measures the performance of a classification model where the prediction is a probability. As we know probability is always between 0, and 1, and multiple probabilities for the same event add up to 1. Log loss increases as the predicted probability diverge from the actual label. Log loss is calculated as: \n",
    "\n",
    "$\\text{Log-loss} = \\frac{-1}{N}\\sum_{i=1}^{N}y_i\\log{p_i} + (1-y_i)\\log(1-p_i)$ \n",
    "\n",
    "**References:**\n",
    "\n",
    "https://towardsdatascience.com/performance-metrics-for-classification-machine-learning-problems-97e7e774a007\n",
    "\n",
    "https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file (not provided, you have to download it by yourself by following the instructions in the module Jupyter notebook). Display the correlation matrix where each row and column are the features, which should be an 8 by 8 matrix (should we use 'Serial no'?). You can use pandas DataFrame.corr() to verify correctness of yours. Observe that the diagonal of this matrix should have all 1's and explain why? Since the last column can be used as the target (dependent) variable, what do you think about the correlations between all the variables? Which variable should be the most important for prediction of 'Chance of Admit'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows=400, M columns=9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "admission = pd.read_csv('../Module 2 Assignment/Admission_Predict.csv')\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(f'N rows={len(admission)}, M columns={len(admission.columns)}')\n",
    "\n",
    "# Check the dataset\n",
    "admission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_numeric(df, exception):\n",
    "    \"\"\" This function takes in a Pandas DataFrame as its parameter\n",
    "        and traverses through the columns coerces each column value\n",
    "        to numeric. Non-numeric values get converted to NaN.\n",
    "        This method returns DataFrame. This method does not discriminate\n",
    "        between nominal, ordinal, and numerical data. \n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if column != exception:\n",
    "            df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check of any missing values\n",
    "admission = force_numeric(admission, \"Chance of Admit\")\n",
    "admission.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that serial number is nominal data. It is simply listed in numbers, and it would not have made much difference if serial number was alphabetical or alphanumeric. Therefore, we can drop this column from our assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission = admission.drop(columns=\"Serial No.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows:\n",
      "[[337.   118.     4.     4.5    4.5    9.65   1.     0.92]\n",
      " [324.   107.     4.     4.     4.5    8.87   1.     0.76]\n",
      " [316.   104.     3.     3.     3.5    8.     1.     0.72]\n",
      " [322.   110.     3.     3.5    2.5    8.67   1.     0.8 ]\n",
      " [314.   103.     2.     2.     3.     8.21   0.     0.65]]\n"
     ]
    }
   ],
   "source": [
    "admission_array = admission.values\n",
    "print(\"First five rows:\" + \"\\n\" + \"{}\".format(admission_array[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation formula is as follows:\n",
    "\n",
    "$r_{xy} = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2\\sum(y_i - \\bar{y})^2}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "   * $r_{xy}$ represents the correlation coefficient of the linear relationship between x and y\n",
    "   * $x_i$ represents the $i^{th}$ value of the x-variable in the sample\n",
    "   * $\\bar{x}$ represents the mean of the values of the x-variable\n",
    "   * $y_i$ represents the $i^{th}$ value of the y-variable in the sample\n",
    "   * $\\bar{y}$ represents the mean of the values of the y-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(array):\n",
    "    \"\"\" This function takes in array and calculates\n",
    "        the correlation matrix. The function assumes\n",
    "        that each row has the same number of columns. \n",
    "        Any missing values values are assumed to be \n",
    "        entred as 0. \n",
    "        \n",
    "        Example: [[1, 2, 3], [4, 6]] is not a valid\n",
    "        parameter. \n",
    "        \n",
    "        Example: [[1, 2, 3], [4, 0, 6]] is a valid\n",
    "        parameter.\n",
    "    \"\"\"\n",
    "    array = np.array(array)\n",
    "    \n",
    "    # calculate covariance matrix                    \n",
    "    covariance_matrix = np.cov(array.T)\n",
    "        \n",
    "    # calculate standard deviation\n",
    "    standard_deviation = np.std(array, axis=0)\n",
    "    \n",
    "    standard_deviation_matrix = []\n",
    "    \n",
    "    k = 0\n",
    "    while k < len(standard_deviation):\n",
    "        std = standard_deviation[k]\n",
    "        arr = []\n",
    "        for i in range(len(standard_deviation)):\n",
    "            multiply = std * standard_deviation[i]\n",
    "            arr.append(multiply)\n",
    "        standard_deviation_matrix.append(arr)\n",
    "        k += 1\n",
    "    \n",
    "    standard_deviation_matrix = np.array(standard_deviation_matrix).T\n",
    "    \n",
    "    correlation_matrix = []\n",
    "    \n",
    "    for i in range(len(standard_deviation_matrix)):\n",
    "        nil = []\n",
    "        for j in range(len(standard_deviation_matrix)):\n",
    "            nil.append(0)\n",
    "        correlation_matrix.append(nil)\n",
    "    \n",
    "    for i in range(len(standard_deviation_matrix)):\n",
    "        for j in range(len(standard_deviation_matrix)):\n",
    "            correlation_matrix[i][j] = round(covariance_matrix[i][j] / standard_deviation_matrix[i][j], 2)\n",
    "    \n",
    "    return np.array(correlation_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = correlation(admission_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.84, 0.67, 0.61, 0.56, 0.84, 0.58, 0.8 ],\n",
       "       [0.84, 1.  , 0.7 , 0.66, 0.57, 0.83, 0.49, 0.79],\n",
       "       [0.67, 0.7 , 1.  , 0.74, 0.66, 0.75, 0.45, 0.71],\n",
       "       [0.61, 0.66, 0.74, 1.  , 0.73, 0.72, 0.45, 0.68],\n",
       "       [0.56, 0.57, 0.66, 0.73, 1.  , 0.67, 0.4 , 0.67],\n",
       "       [0.84, 0.83, 0.75, 0.72, 0.67, 1.  , 0.52, 0.88],\n",
       "       [0.58, 0.49, 0.45, 0.45, 0.4 , 0.52, 1.  , 0.55],\n",
       "       [0.8 , 0.79, 0.71, 0.68, 0.67, 0.88, 0.55, 1.  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE Score</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835977</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.802610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL Score</th>\n",
       "      <td>0.835977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.791594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.695590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.711250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.612831</td>\n",
       "      <td>0.657981</td>\n",
       "      <td>0.734523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.675732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.567721</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.729593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.669889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.833060</td>\n",
       "      <td>0.828417</td>\n",
       "      <td>0.746479</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.670211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>0.873289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.444029</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.521654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance of Admit</th>\n",
       "      <td>0.802610</td>\n",
       "      <td>0.791594</td>\n",
       "      <td>0.711250</td>\n",
       "      <td>0.675732</td>\n",
       "      <td>0.669889</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.553202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
       "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
       "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
       "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
       "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
       "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
       "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
       "Research            0.580391     0.489858           0.447783  0.444029   \n",
       "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
       "\n",
       "                       LOR       CGPA  Research  Chance of Admit   \n",
       "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
       "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
       "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
       "SOP                0.729593  0.718144  0.444029          0.675732  \n",
       "LOR                1.000000  0.670211  0.396859          0.669889  \n",
       "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
       "Research           0.396859  0.521654  1.000000          0.553202  \n",
       "Chance of Admit    0.669889  0.873289  0.553202          1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have rounded the numbers of correlation matrix to two decimal places to present in a concise manner, however we can see that the diagonal of the matrix is all 1's, and other correlations also match with the ones provided by pandas library. If a student wants highest chance of admittance, then they need to focus on CGPA, and GRE Score to along with it. Notice that the Chance of Admit is 80% with GRE Score, and 87% with CGPA. However the variabe that should be most important is a student's CGPA. A correlation of 1 along the diagonal means perfect correlation, but notice that the diagonals are where a feature has a correlation with itself, and naturally a feature is perfectly correlated with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
