{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 9 | Applied Machine Learning | Paras Ahuja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please refer to the Reinforcement Learning Jupyter notebook in course materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describe the environment in the Nim learning model.\n",
    "\n",
    "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharat state in their paper entitled \"Deep Reinforcement Learning: A brief survey \": in the  RL  setup,  an  autonomous  agent,  controlled  by  a  machine-learning algorithm, observes a statest from its environment at time step $t$ The agent interacts with the environment by taking an action $a_t$ in state $s_t$. When the agent takes an action, the environment and the agent transition to a new state, $s_{t+1}$ based on the current state and the chosen action. \n",
    "\n",
    "**The environment in the game of Nim is the board itself.** In the Nim learning model we have the Nim game of strategy between two players in which players remove items from three piles at each turn. Every turn the player must remove at least one item from a pile. There are two versions of the game goal: \n",
    "1. the player who clears the last pile wins\n",
    "2. the player who clears the last pile (by removing >1 items) loses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe the agent in the Nim learning model.\n",
    "\n",
    "The goal of the agent is to learn a policy(control strategy) $\\pi$ that maximizes the expected return (cumulative, discounted reward). Our agent is Q-Learning algorithm or the Q-Learning agent. At the start of the game or training phase the entries in the agent’s Q-table are zero. The agent is  programmed to read the board and choose an action according to the algorithm. Generally speaking, if the procedure produces an illegal move then the agent should make a random legal move instead. Essentially, the agent is using the qtable for expected payoffs to make guesses about the stack. Our agent is programmed to gravitate towards the highest expected reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Describe the reward and penalty in the Nim learning model.\n",
    "\n",
    "The reward is given to the agent after each pair of moves: the agent moves, then the opponent moves, in this way the agent learns the ultimate consequences of its actions and the agent's qtable is updated accordingly. If on the other hand the opponent made the first move of the game, the agent’s Q-table is first updated after the opponent made its second move. Generally speaking in many Q-learning implementations, our goal is to learn a reward value for every (state, action) pair. An action that loses the game will have a reward of -1, an action that results in the other player losing the game will have a reward of 1, and an action that results in the game continuing has an immediate reward of 0, but will also have some future reward.\n",
    "\n",
    "Reference:https://cs50.harvard.edu/extension/ai/2020/spring/projects/4/nim/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?\n",
    "\n",
    "A state is any permutation of the board. The number of individual states possible in a game of Nim can be easily calculated. For a board containing three heaps of $x_1, x_2, \\text{and} x_3$ counters the total number of states is given by $(x_1+1)(x_2+1)(x_3 +1)$. Then, we have $(10+1)(10+1)(10+1) = 11^3 = 1331.$ Therefore, the game has 1331 states - thats a lot of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How many possible actions there could be in the Nim game with 10 items per pile and 3 piles total?\n",
    "\n",
    "Agents interact with the environment by removing counters from the board. From a heap of x counters an agent can take anything from 1 up to x counters. The agent must however only take counters from one heap in each move. The total number of actions available to the agent is therefore equal to $x_1 + x_2 + x_3$. Then, we have $10 + 10 + 10 = 30$. Therefore, we have 30 possible actions in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find a way to improve the provided Nim game learning model. Do you think one can beat the Guru player? (Hint: How about penalizing the losses? Hint: It is indeed possible to find a better solution, which improves the way Q-learning updates its Q-table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is entirely possible for any of the 3 agent to learn the optimal strategy of Nim. We notice that the strategy below improves the algorithm by a bit as compared to default. This means that there is definitely a method will can make the algorithm learn the policy that is highly comparable to guru learner. Furthermore, we have to note the point that if Guru goes first, it will win vast majority of the time, at least with this iteration of the algorithm. However, if Qlearner goes first, it wins more than it would otherwise. We notice that in one iteration of the game, Qlearner manages to win 105 times against Guru learner, however Qlearner went first. Still, there is more room for improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# Who won\n",
    "won = \"Guru\"\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s} {wins['A']}   {b:>8s} {wins['B']}\")\n",
    "    #\n",
    "    won = a if wins['A'] > wins['B'] else b\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,   Random 492     Random 508\n",
      "1000 games,     Guru 999     Random 1\n",
      "1000 games,   Random 6       Guru 994\n",
      "1000 games,     Guru 950       Guru 50\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Random', 'Random')\n",
    "play_games(1000, 'Guru', 'Random')\n",
    "play_games(1000, 'Random', 'Guru')\n",
    "play_games(1000, 'Guru', 'Guru') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            nim_sum = st2[0] ^ st2[1] ^ st2[2]\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game  \n",
    "            elif nim_sum != 0:\n",
    "                qtable_update(-Reward, st1, move, pile, np.min(qtable[st2[0], st2[1], st2[2]]))\n",
    "            else:\n",
    "                qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.75 ms, sys: 2.19 ms, total: 8.94 ms\n",
      "Wall time: 6.73 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nim_qlearn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner 731     Random 269\n",
      "1000 games,   Random 263   Qlearner 737\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,     Guru 998   Qlearner 2\n",
      "1000 games,     Guru 999   Qlearner 1\n",
      "1000 games,     Guru 998   Qlearner 2\n",
      "1000 games,     Guru 995   Qlearner 5\n",
      "1000 games,     Guru 991   Qlearner 9\n",
      "1000 games,     Guru 994   Qlearner 6\n",
      "1000 games,     Guru 989   Qlearner 11\n",
      "CPU times: user 7.5 s, sys: 838 ms, total: 8.34 s\n",
      "Wall time: 7.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'Guru', 'Qlearner')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.998, 0.999, 0.998, 0.995, 0.991, 0.994, 0.989]\n"
     ]
    }
   ],
   "source": [
    "# Check the ratio of wins wrt to size of the reinforcement model training\n",
    "print(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        #\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            #\n",
    "            print(s, file=fout)\n",
    "#\n",
    "qtable_log('qtable_debug.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
